# -*- coding: utf-8 -*-
"""Image-Captioning2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rKB87Beei8BxnYIH0WlEMg6WXfgDnE3o
"""

from PIL import Image
import torch
from transformers import Blip2Processor, Blip2ForConditionalGeneration

device = "cuda" if torch.cuda.is_available() else "cpu"
processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    torch_dtype=torch.float16 if device == "cuda" else torch.float32
).to(device)

image = Image.open("example.jpg").convert("RGB")
prompt = "Describe this image in detail."

inputs = processor(image, prompt, return_tensors="pt").to(device)
generated_ids = model.generate(**inputs, max_new_tokens=50)
caption = processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True)

print("Caption:", caption)